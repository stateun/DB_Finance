{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('./data/test1_results.csv')\n",
    "test2 = pd.read_csv('./data/test2_results.csv')\n",
    "test3 = pd.read_csv('./data/test3_results.csv')\n",
    "test4 = pd.read_csv('./data/test4_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = test4['model'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, We check the maximum value for each methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNNet1 = test1[test1['model'] == 'IGNnet']\n",
    "IGNNet1[IGNNet1['ACC'] >= 88].sort_values(by=['AUC', 'F1_score'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNNet2 = test2[test2['model'] == 'IGNnet']\n",
    "IGNNet2[IGNNet2['ACC'] >= 88].sort_values(by=['AUC', 'F1_score'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNNet3 = test3[test3['model'] == 'IGNnet']\n",
    "IGNNet3[IGNNet3['ACC'] >= 90].sort_values(by=['AUC', 'F1_score'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNNet4 = test4[test4['model'] == 'IGNnet']\n",
    "IGNNet4[IGNNet4['ACC'] >= 90].sort_values(by=['AUC', 'F1_score'], ascending=[False, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve hyperparameters for reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(IGNNet4[IGNNet4['AUC'] == 82]['best_params'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_index = IGNNet4[IGNNet4['ACC'] >= 90].sort_values(by=['AUC', 'F1_score'], ascending=[False, False]).index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_traditional = len(model_list)\n",
    "len_traditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, We compare the performance between IGNnet and traditional methods (e.g. LR, DT, SVM ...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_perform = test4.iloc[num_index:num_index + len_traditional , 1:]\n",
    "get_perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNNet4['best_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, We compare the performance between methods about oversamplings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_methods = [\"borderline-smote\", \"smote\", \"adasyn\", \"over-random\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_data1 = {method: IGNNet1.loc[IGNNet1[\"best_params\"].str.contains(f\"sampling:{method}\", na=False)]\n",
    "                 for method in sampling_methods}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_df = []\n",
    "\n",
    "for sampling in sampling_methods :\n",
    "    stack = over_data1[sampling].sort_values(by = ['AUC', 'F1_score'], ascending=[False, False]).iloc[0]\n",
    "    perform_df.append(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_df = pd.DataFrame([s.to_dict() for s in perform_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_data2 = {method: IGNNet2.loc[IGNNet2[\"best_params\"].str.contains(f\"sampling:{method}\", na=False)]\n",
    "                 for method in sampling_methods}\n",
    "\n",
    "perform_df2 = []\n",
    "\n",
    "for sampling in sampling_methods :\n",
    "    stack = over_data2[sampling].sort_values(by = ['AUC', 'F1_score'], ascending=[False, False]).iloc[0]\n",
    "    perform_df2.append(stack)\n",
    "    \n",
    "perform_df2 = pd.DataFrame([s.to_dict() for s in perform_df2])\n",
    "perform_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_data3 = {method: IGNNet3.loc[IGNNet3[\"best_params\"].str.contains(f\"sampling:{method}\", na=False)]\n",
    "                 for method in sampling_methods}\n",
    "\n",
    "perform_df3 = []\n",
    "\n",
    "for sampling in sampling_methods :\n",
    "    stack = over_data3[sampling].sort_values(by = ['AUC', 'F1_score'], ascending=[False, False]).iloc[0]\n",
    "    perform_df3.append(stack)\n",
    "    \n",
    "perform_df3 = pd.DataFrame([s.to_dict() for s in perform_df3])\n",
    "perform_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_data4 = {method: IGNNet4.loc[IGNNet4[\"best_params\"].str.contains(f\"sampling:{method}\", na=False)]\n",
    "                 for method in sampling_methods}\n",
    "\n",
    "perform_df4 = []\n",
    "\n",
    "for sampling in sampling_methods :\n",
    "    stack = over_data4[sampling].sort_values(by = ['AUC', 'F1_score'], ascending=[False, False]).iloc[0]\n",
    "    perform_df4.append(stack)\n",
    "    \n",
    "perform_df4 = pd.DataFrame([s.to_dict() for s in perform_df4])\n",
    "perform_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_all = pd.concat([perform_df, perform_df2, perform_df3, perform_df4], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALL Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_all['sampling_method'] = perform_all['best_params'].apply(lambda x: next((s for s in sampling_methods if s in str(x)), None))\n",
    "\n",
    "best_auc_idx = perform_all.groupby(\"sampling_method\")[\"AUC\"].idxmax()\n",
    "best_f1_idx = perform_all.groupby(\"sampling_method\")[\"F1_score\"].idxmax()\n",
    "best_auc = perform_all.loc[best_auc_idx]\n",
    "best_f1 = perform_all.loc[best_f1_idx]\n",
    "\n",
    "merged_perform_all = pd.concat([best_auc, best_f1]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "final_rows = []\n",
    "for method in sampling_methods:\n",
    "    subset = merged_perform_all[merged_perform_all['sampling_method'] == method]\n",
    "\n",
    "    if len(subset) > 1:\n",
    "        row1, row2 = subset.iloc[0], subset.iloc[1]\n",
    "        if row1[\"AUC\"] > row2[\"AUC\"] and row1[\"F1_score\"] > row2[\"F1_score\"]:\n",
    "            final_rows.append(row1)\n",
    "        elif row2[\"AUC\"] > row1[\"AUC\"] and row2[\"F1_score\"] > row1[\"F1_score\"]:\n",
    "            final_rows.append(row2)\n",
    "        else:\n",
    "            subset = subset.copy() \n",
    "            subset[\"mean_score\"] = (subset[\"AUC\"] + subset[\"F1_score\"]) / 2\n",
    "            chosen_row = subset.sort_values(\"mean_score\", ascending=False).iloc[0]\n",
    "            final_rows.append(chosen_row)\n",
    "    elif len(subset) == 1:\n",
    "        final_rows.append(subset.iloc[0])\n",
    "\n",
    "final_perform_all = pd.DataFrame(final_rows).reset_index(drop=True).drop(['dataset', 'best_params', 'mean_score'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_perform_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
